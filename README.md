# Meta's LLaMA-3 From Scratch

This repository is an implementation of Meta's LLaMA-3, especially 8B model from scratch.

## Repository Structure

The repository contains the following files:

- `config.json`: Configuration file.
- `data_preprocessing.py`: Script for data preprocessing.
- `generation_config.json`: Configuration file for generation.
- `inference.py`: Script for performing inference.
- `model.py`: Implementation of the LLaMA-3 model.
- `pretraining.py`: Script for pretraining the model.
- `README.md`: This file, providing documentation for the repository.
- `requirements.txt`: List of dependencies.
- `special_tokens_map.json`: Mapping of special tokens.
- `tokenizer_config.json`: Configuration file for the tokenizer.
- `tokenizer.py`: Implementation of the tokenizer.
- `llama3/tokenizer.model`: Tokenizer model file.

## Original Model Weights

For Original Model Weights, please visit the following link:

```markdown
https://huggingface.co/meta-llama/Meta-Llama-3-8B/resolve/main/original/consolidated.00.pth
